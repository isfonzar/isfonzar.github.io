I"ð]<h2 id="kubeconfigs">Kubeconfigs</h2>

<p>Kubeconfigs are Kubernetes configuration file, it stores information about clusters, users, namespaces and authentication mechanisms. It contains the configuration data that we need in order to connect and to interact with a Kubernetes cluster.</p>

<h2 id="requirements">Requirements</h2>

<p>To generate the kubeconfig files, we need to have the <code class="highlighter-rouge">kubectl</code> tool installed</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>wget https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl
<span class="nv">$ </span><span class="nb">chmod</span> +x kubectl
<span class="nv">$ </span><span class="nb">sudo mv </span>kubectl /usr/local/bin/
<span class="nv">$ </span>kubectl version <span class="nt">--client</span> <span class="c"># check the installation</span>
</code></pre></div></div>

<h2 id="generating-the-required-kubeconfigs">Generating the required kubeconfigs</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Ip address of the Kubernetes API</span>
<span class="nv">KUBERNETES_ADDRESS</span><span class="o">=</span>172.34.2.0

<span class="nv">WORKER0_HOST</span><span class="o">=</span>worker0.mylabserver.com
<span class="nv">WORKER0_IP</span><span class="o">=</span>172.34.1.0
<span class="nv">WORKER1_HOST</span><span class="o">=</span>worker1.mylabserver.com
<span class="nv">WORKER1_IP</span><span class="o">=</span>172.34.1.1

<span class="nv">CONTROLLER0_IP</span><span class="o">=</span>172.34.0.0
<span class="nv">CONTROLLER1_IP</span><span class="o">=</span>172.34.0.1

<span class="c"># Specify the user you will be connecting to those servers with</span>
<span class="nv">CLOUD_USER</span><span class="o">=</span>cloud_user
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for </span>instance <span class="k">in</span> <span class="k">${</span><span class="nv">WORKER0_HOST</span><span class="k">}</span> <span class="k">${</span><span class="nv">WORKER1_HOST</span><span class="k">}</span><span class="p">;</span> <span class="k">do
  </span>kubectl config set-cluster kubernetes-the-hard-way <span class="se">\</span>
    <span class="nt">--certificate-authority</span><span class="o">=</span>ca.pem <span class="se">\</span>
    <span class="nt">--embed-certs</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
    <span class="nt">--server</span><span class="o">=</span>https://<span class="k">${</span><span class="nv">KUBERNETES_ADDRESS</span><span class="k">}</span>:6443 <span class="se">\</span>
    <span class="nt">--kubeconfig</span><span class="o">=</span><span class="k">${</span><span class="nv">instance</span><span class="k">}</span>.kubeconfig

  kubectl config set-credentials system:node:<span class="k">${</span><span class="nv">instance</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--client-certificate</span><span class="o">=</span><span class="k">${</span><span class="nv">instance</span><span class="k">}</span>.pem <span class="se">\</span>
    <span class="nt">--client-key</span><span class="o">=</span><span class="k">${</span><span class="nv">instance</span><span class="k">}</span><span class="nt">-key</span>.pem <span class="se">\</span>
    <span class="nt">--embed-certs</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
    <span class="nt">--kubeconfig</span><span class="o">=</span><span class="k">${</span><span class="nv">instance</span><span class="k">}</span>.kubeconfig

  kubectl config set-context default <span class="se">\</span>
    <span class="nt">--cluster</span><span class="o">=</span>kubernetes-the-hard-way <span class="se">\</span>
    <span class="nt">--user</span><span class="o">=</span>system:node:<span class="k">${</span><span class="nv">instance</span><span class="k">}</span> <span class="se">\</span>
    <span class="nt">--kubeconfig</span><span class="o">=</span><span class="k">${</span><span class="nv">instance</span><span class="k">}</span>.kubeconfig

  kubectl config use-context default <span class="nt">--kubeconfig</span><span class="o">=</span><span class="k">${</span><span class="nv">instance</span><span class="k">}</span>.kubeconfig
<span class="k">done</span>
</code></pre></div></div>

<p>Now, we generate the kubeconfig for kubeproxy. Only 1 file is needed for all the workers as they will share this file.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl config set-cluster kubernetes-the-hard-way <span class="se">\</span>
    <span class="nt">--certificate-authority</span><span class="o">=</span>ca.pem <span class="se">\</span>
    <span class="nt">--embed-certs</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
    <span class="nt">--server</span><span class="o">=</span>https://<span class="k">${</span><span class="nv">KUBERNETES_ADDRESS</span><span class="k">}</span>:6443 <span class="se">\</span>
    <span class="nt">--kubeconfig</span><span class="o">=</span>kube-proxy.kubeconfig

  kubectl config set-credentials system:kube-proxy <span class="se">\</span>
    <span class="nt">--client-certificate</span><span class="o">=</span>kube-proxy.pem <span class="se">\</span>
    <span class="nt">--client-key</span><span class="o">=</span>kube-proxy-key.pem <span class="se">\</span>
    <span class="nt">--embed-certs</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
    <span class="nt">--kubeconfig</span><span class="o">=</span>kube-proxy.kubeconfig

  kubectl config set-context default <span class="se">\</span>
    <span class="nt">--cluster</span><span class="o">=</span>kubernetes-the-hard-way <span class="se">\</span>
    <span class="nt">--user</span><span class="o">=</span>system:kube-proxy <span class="se">\</span>
    <span class="nt">--kubeconfig</span><span class="o">=</span>kube-proxy.kubeconfig

  kubectl config use-context default <span class="nt">--kubeconfig</span><span class="o">=</span>kube-proxy.kubeconfig
</code></pre></div></div>

<p>Now, for the kube controller manager, here we use the default IP for localhost, the kube-controller-manager will not go through the load-balancer, but will access API from whatever host it is in.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl config set-cluster kubernetes-the-hard-way <span class="se">\</span>
    <span class="nt">--certificate-authority</span><span class="o">=</span>ca.pem <span class="se">\</span>
    <span class="nt">--embed-certs</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
    <span class="nt">--server</span><span class="o">=</span>https://127.0.0.1:6443 <span class="se">\</span>
    <span class="nt">--kubeconfig</span><span class="o">=</span>kube-controller-manager.kubeconfig

  kubectl config set-credentials system:kube-controller-manager <span class="se">\</span>
    <span class="nt">--client-certificate</span><span class="o">=</span>kube-controller-manager.pem <span class="se">\</span>
    <span class="nt">--client-key</span><span class="o">=</span>kube-controller-manager-key.pem <span class="se">\</span>
    <span class="nt">--embed-certs</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
    <span class="nt">--kubeconfig</span><span class="o">=</span>kube-controller-manager.kubeconfig

  kubectl config set-context default <span class="se">\</span>
    <span class="nt">--cluster</span><span class="o">=</span>kubernetes-the-hard-way <span class="se">\</span>
    <span class="nt">--user</span><span class="o">=</span>system:kube-controller-manager <span class="se">\</span>
    <span class="nt">--kubeconfig</span><span class="o">=</span>kube-controller-manager.kubeconfig

  kubectl config use-context default <span class="nt">--kubeconfig</span><span class="o">=</span>kube-controller-manager.kubeconfig
</code></pre></div></div>

<p>The kube-scheduler kubeconfig:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl config set-cluster kubernetes-the-hard-way <span class="se">\</span>
    <span class="nt">--certificate-authority</span><span class="o">=</span>ca.pem <span class="se">\</span>
    <span class="nt">--embed-certs</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
    <span class="nt">--server</span><span class="o">=</span>https://127.0.0.1:6443 <span class="se">\</span>
    <span class="nt">--kubeconfig</span><span class="o">=</span>kube-scheduler.kubeconfig

  kubectl config set-credentials system:kube-scheduler <span class="se">\</span>
    <span class="nt">--client-certificate</span><span class="o">=</span>kube-scheduler.pem <span class="se">\</span>
    <span class="nt">--client-key</span><span class="o">=</span>kube-scheduler-key.pem <span class="se">\</span>
    <span class="nt">--embed-certs</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
    <span class="nt">--kubeconfig</span><span class="o">=</span>kube-scheduler.kubeconfig

  kubectl config set-context default <span class="se">\</span>
    <span class="nt">--cluster</span><span class="o">=</span>kubernetes-the-hard-way <span class="se">\</span>
    <span class="nt">--user</span><span class="o">=</span>system:kube-scheduler <span class="se">\</span>
    <span class="nt">--kubeconfig</span><span class="o">=</span>kube-scheduler.kubeconfig

  kubectl config use-context default <span class="nt">--kubeconfig</span><span class="o">=</span>kube-scheduler.kubeconfig
</code></pre></div></div>

<p>And lastly for the admin user:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl config set-cluster kubernetes-the-hard-way <span class="se">\</span>
    <span class="nt">--certificate-authority</span><span class="o">=</span>ca.pem <span class="se">\</span>
    <span class="nt">--embed-certs</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
    <span class="nt">--server</span><span class="o">=</span>https://127.0.0.1:6443 <span class="se">\</span>
    <span class="nt">--kubeconfig</span><span class="o">=</span>admin.kubeconfig

  kubectl config set-credentials admin <span class="se">\</span>
    <span class="nt">--client-certificate</span><span class="o">=</span>admin.pem <span class="se">\</span>
    <span class="nt">--client-key</span><span class="o">=</span>admin-key.pem <span class="se">\</span>
    <span class="nt">--embed-certs</span><span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
    <span class="nt">--kubeconfig</span><span class="o">=</span>admin.kubeconfig

  kubectl config set-context default <span class="se">\</span>
    <span class="nt">--cluster</span><span class="o">=</span>kubernetes-the-hard-way <span class="se">\</span>
    <span class="nt">--user</span><span class="o">=</span>admin <span class="se">\</span>
    <span class="nt">--kubeconfig</span><span class="o">=</span>admin.kubeconfig

  kubectl config use-context default <span class="nt">--kubeconfig</span><span class="o">=</span>admin.kubeconfig
</code></pre></div></div>

<p>If you open one of those files we just generate, you can see the certificate authority, the server (load balancer IP), username and certificates for the client. All the information we generated on Part 1 is now present within those files.</p>

<h2 id="distributing-the-kubeconfig-files">Distributing the kubeconfig files</h2>

<p>Moving the kubeconfig to the worker nodes</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scp <span class="k">${</span><span class="nv">WORKER0_HOST</span><span class="k">}</span>.kubeconfig kube-proxy.kubeconfig <span class="k">${</span><span class="nv">CLOUD_USER</span><span class="k">}</span>@<span class="k">${</span><span class="nv">WORKER0_IP</span><span class="k">}</span>:~/
scp <span class="k">${</span><span class="nv">WORKER1_HOST</span><span class="k">}</span>.kubeconfig kube-proxy.kubeconfig <span class="k">${</span><span class="nv">CLOUD_USER</span><span class="k">}</span>@<span class="k">${</span><span class="nv">WORKER1_IP</span><span class="k">}</span>:~/
</code></pre></div></div>

<p>Moving the kubeconfig files to the controller nodes:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig <span class="k">${</span><span class="nv">CLOUD_USER</span><span class="k">}</span>@<span class="k">${</span><span class="nv">CONTROLLER0_IP</span><span class="k">}</span>:~/
scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig <span class="k">${</span><span class="nv">CLOUD_USER</span><span class="k">}</span>@<span class="k">${</span><span class="nv">CONTROLLER1_IP</span><span class="k">}</span>:~/
</code></pre></div></div>

<h2 id="generating-the-kubernetes-data-encryption-config">Generating the Kubernetes Data Encryption Config</h2>

<p>Kubernetes supports the ability to encrypt secret data at rest, so any sensitive data is always encrypted. In order to make use of this feature, we need to provide kubernetes with an encryption key.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">CONTROLLER0_IP</span><span class="o">=</span>10.0.1.39
<span class="nv">CONTROLLER1_IP</span><span class="o">=</span>10.0.1.90

<span class="nv">CLOUD_USER</span><span class="o">=</span>cloud_user

<span class="c"># Generate a random encryption key</span>
<span class="nv">ENCRYPTION_KEY</span><span class="o">=</span><span class="si">$(</span><span class="nb">head</span> <span class="nt">-c</span> 32 /dev/urandom | <span class="nb">base64</span><span class="si">)</span>
</code></pre></div></div>

<p>Now, letâ€™s create the encryption config file:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&gt;</span> encryption-config.yaml <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: </span><span class="k">${</span><span class="nv">ENCRYPTION_KEY</span><span class="k">}</span><span class="sh">
      - identity: {}
</span><span class="no">EOF
</span></code></pre></div></div>

<p>And lastly, letâ€™s copy this encryption config file to the controller servers.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scp encryption-config.yaml <span class="k">${</span><span class="nv">CLOUD_USER</span><span class="k">}</span>@<span class="k">${</span><span class="nv">CONTROLLER0_IP</span><span class="k">}</span>:~/
scp encryption-config.yaml <span class="k">${</span><span class="nv">CLOUD_USER</span><span class="k">}</span>@<span class="k">${</span><span class="nv">CONTROLLER1_IP</span><span class="k">}</span>:~/
</code></pre></div></div>

<h2 id="bootstrapping-the-etcd-cluster">Bootstrapping the etcd cluster</h2>

<p><a href="https://github.com/coreos/etcd">etcd</a> is a distributed key value store that provides a reliable way to store data across a cluster of machines.</p>

<p>It provides a way to store data across a distributed cluster of machines and make sure the data is synchronized across all machines.</p>

<p>Kubernetes uses etcd to store all of its internal data about cluster state.</p>

<p>This data needs to be stored, but it also needs to be reliably synchronized across all controller nodes in the cluster.</p>

<p>We just need to install it on our controller nodes.</p>

<p>More information on the official kubernetes documentation on <a href="https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/">configuring etcd</a></p>

<h3 id="creating-the-etcd-cluster">Creating the etcd cluster</h3>

<p>Log in both of the controller servers, for this you are going to execute commands in both of the controllers.</p>

<p>First, letâ€™s install <code class="highlighter-rouge">etcd</code> for both of the controllers</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Download binaries</span>
wget <span class="nt">-q</span> <span class="nt">--show-progress</span> <span class="nt">--https-only</span> <span class="nt">--timestamping</span> <span class="se">\</span>
  <span class="s2">"https://github.com/coreos/etcd/releases/download/v3.3.5/etcd-v3.3.5-linux-amd64.tar.gz"</span>

<span class="c"># Extract the contents</span>
<span class="nb">tar</span> <span class="nt">-xvf</span> etcd-v3.3.5-linux-amd64.tar.gz

<span class="c"># Move the executables</span>
<span class="nb">sudo mv </span>etcd-v3.3.5-linux-amd64/etcd<span class="k">*</span> /usr/local/bin/

<span class="c"># Create some necessaries directories to run etcd</span>
<span class="nb">sudo mkdir</span> <span class="nt">-p</span> /etc/etcd /var/lib/etcd

<span class="c"># Move the certificates file into the correct location</span>
<span class="c"># certificate authority, kubernetes certificate key and pem</span>
<span class="c"># don't move them, but copy, as they will be used later</span>
<span class="nb">sudo cp </span>ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/
</code></pre></div></div>

<p>Now, we need to create the systemd file to etcd. Letâ€™s make some environment variables to make it easier:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">CONTROLLER0_HOST</span><span class="o">=</span>controller0.host
<span class="nv">CONTROLLER0_IP</span><span class="o">=</span>10.0.1.55
<span class="nv">CONTROLLER1_HOST</span><span class="o">=</span>controller1.host
<span class="nv">CONTROLLER1_IP</span><span class="o">=</span>10.0.1.196

<span class="c"># has to be different for each controller</span>
<span class="nv">ETCD_NAME</span><span class="o">=</span>controller1.host
<span class="c"># using the hostname might make it easier to identify</span>

<span class="c"># internal/private ip of each of the servers</span>
<span class="nv">INTERNAL_IP</span><span class="o">=</span><span class="si">$(</span>curl http://169.254.169.254/latest/meta-data/local-ipv4<span class="si">)</span>
<span class="c"># this only works because the server's are amazon ec2 instances</span>

<span class="c"># all the servers in the cluster</span>
<span class="nv">INITIAL_CLUSTER</span><span class="o">=</span><span class="k">${</span><span class="nv">CONTROLLER0_HOST</span><span class="k">}</span><span class="o">=</span>https://<span class="k">${</span><span class="nv">CONTROLLER0_IP</span><span class="k">}</span>:2380,<span class="k">${</span><span class="nv">CONTROLLER1_HOST</span><span class="k">}</span><span class="o">=</span>https://<span class="k">${</span><span class="nv">CONTROLLER1_IP</span><span class="k">}</span>:2380
</code></pre></div></div>

<p>Then, create the systemd unit file</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> | sudo tee /etc/systemd/system/etcd.service
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
ExecStart=/usr/local/bin/etcd </span><span class="se">\\</span><span class="sh">
  --name </span><span class="k">${</span><span class="nv">ETCD_NAME</span><span class="k">}</span><span class="sh"> </span><span class="se">\\</span><span class="sh">
  --cert-file=/etc/etcd/kubernetes.pem </span><span class="se">\\</span><span class="sh">
  --key-file=/etc/etcd/kubernetes-key.pem </span><span class="se">\\</span><span class="sh">
  --peer-cert-file=/etc/etcd/kubernetes.pem </span><span class="se">\\</span><span class="sh">
  --peer-key-file=/etc/etcd/kubernetes-key.pem </span><span class="se">\\</span><span class="sh">
  --trusted-ca-file=/etc/etcd/ca.pem </span><span class="se">\\</span><span class="sh">
  --peer-trusted-ca-file=/etc/etcd/ca.pem </span><span class="se">\\</span><span class="sh">
  --peer-client-cert-auth </span><span class="se">\\</span><span class="sh">
  --client-cert-auth </span><span class="se">\\</span><span class="sh">
  --initial-advertise-peer-urls https://</span><span class="k">${</span><span class="nv">INTERNAL_IP</span><span class="k">}</span><span class="sh">:2380 </span><span class="se">\\</span><span class="sh">
  --listen-peer-urls https://</span><span class="k">${</span><span class="nv">INTERNAL_IP</span><span class="k">}</span><span class="sh">:2380 </span><span class="se">\\</span><span class="sh">
  --listen-client-urls https://</span><span class="k">${</span><span class="nv">INTERNAL_IP</span><span class="k">}</span><span class="sh">:2379,https://127.0.0.1:2379 </span><span class="se">\\</span><span class="sh">
  --advertise-client-urls https://</span><span class="k">${</span><span class="nv">INTERNAL_IP</span><span class="k">}</span><span class="sh">:2379 </span><span class="se">\\</span><span class="sh">
  --initial-cluster-token etcd-cluster-0 </span><span class="se">\\</span><span class="sh">
  --initial-cluster </span><span class="k">${</span><span class="nv">INITIAL_CLUSTER</span><span class="k">}</span><span class="sh"> </span><span class="se">\\</span><span class="sh">
  --initial-cluster-state new </span><span class="se">\\</span><span class="sh">
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
</span><span class="no">EOF
</span></code></pre></div></div>

<p>After the creation of the systemd unit file, itâ€™s necessary to start and enable the etcd service:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># necessary to do every time we modify/add a new unit file, so systemd see the changes</span>
<span class="nb">sudo </span>systemctl daemon-reload 

<span class="c"># start automatically every time the server comes up</span>
<span class="nb">sudo </span>systemctl <span class="nb">enable </span>etcd

<span class="c"># start the etcd service</span>
<span class="nb">sudo </span>systemctl start etcd
</code></pre></div></div>

<p>Verify if itâ€™s working by running:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># check if etcd is running</span>
<span class="nb">sudo </span>systemctl status etcd

<span class="c"># get a list of all the etcd hosts</span>
<span class="nb">sudo </span><span class="nv">ETCDCTL_API</span><span class="o">=</span>3 etcdctl member list <span class="se">\</span>
  <span class="nt">--endpoints</span><span class="o">=</span>https://127.0.0.1:2379 <span class="se">\</span>
  <span class="nt">--cacert</span><span class="o">=</span>/etc/etcd/ca.pem <span class="se">\</span>
  <span class="nt">--cert</span><span class="o">=</span>/etc/etcd/kubernetes.pem <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/etcd/kubernetes-key.pem
</code></pre></div></div>

<p>The last command should display both servers, showing that the etcd cluster has been created correctly.</p>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/">Organizing Cluster Access Using kubeconfig Files</a></li>
</ul>
:ET